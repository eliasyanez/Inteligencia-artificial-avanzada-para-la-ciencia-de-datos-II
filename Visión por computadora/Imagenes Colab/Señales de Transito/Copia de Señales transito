{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNDoyllRDWcArvMetZ8Zftl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Criterios:\n","\n","Separación de datos en carpetas\n","Separa los datos de forma que se puedan alimentar al modelo usando los dataloaders.\n","\n","Este criterio depende de una competencia de aprendizaje Mostrar una imagen del data set\n","Mostrar una imagen utilizando matplotlib.\n","\n","Definición del modelo\n","Crear un modelo utilizando capas convolucionales. Se considera como input una imagen, y como salida cada una de las categorías. Proponer funciones de activación, optimizador, etc. Mostrar el resumen del modelo. Se utiliza transfer learning.\n","\n","Aumento de datos\n","Crear un image data generator con transformaciones adecuadas para el data set, para aplicarlo posteriormente como aumento de datos.\n","\n","Generadores de datos\n","Crear generadores para entrenamiento, validación, y pruebas.\n","\n","Visualización de datos del generador\n","Mostrar una figura con 20 imágenes creadas por el generador de datos de pruebas.\n","\n","Entrenamiento.\n","Entrenar el modelo con los datos de los generadores.\n","\n","Evaluación.\n","Haz predicciones sobre los datos de prueba.\n","\n","Graficar resultados.\n","Graficar el accuracy y el loss del entrenamiento.\n","\n","Pruebas de predicción con imágenes nuevas\n","Agrega predicciones con imágenes de prueba que no sean parte del dataset.\n","\n","\n","Pasos:\n","\n","Paso 1: Importar bibliotecas\n","\n","Importa las bibliotecas necesarias, incluyendo PyTorch, Torchvision, Lightning, etc. para el aprendizaje profundo, la visualización y el análisis de datos.\n","\n","Paso 2: Cargar y visualizar los datos CIFAR-10\n","\n","Carga los conjuntos de datos de entrenamiento y prueba CIFAR-10 usando Torchvision.\n","Divide el conjunto de datos de entrenamiento en conjuntos de entrenamiento y validación.\n","Crea dataloaders para un manejo eficiente de los datos durante el entrenamiento.\n","Visualiza imágenes de muestra del conjunto de datos de entrenamiento.\n","\n","Paso 3: Construir y entrenar un modelo CNN básico\n","\n","Define un modelo CNN usando un modelo ResNet-18 preentrenado de PyTorch Hub.\n","Crea una clase LightningModule para encapsular el modelo, las funciones de entrenamiento y las métricas.\n","Configura el proceso de entrenamiento usando un entrenador Lightning.\n","Entrena el modelo en los datos de entrenamiento y evalúa el rendimiento en el conjunto de validación.\n","\n","Paso 4: Mejorar la precisión del modelo\n","\n","Explora diferentes técnicas para mejorar la precisión del modelo, incluyendo la transferencia de aprendizaje y el aumento de datos.\n","Implementa el aumento de datos usando transformaciones de Torchvision para aumentar la variabilidad de los datos de entrenamiento.\n","Ajusta los hiperparámetros y entrena el modelo usando el conjunto de datos aumentado.\n","Evalúa el modelo mejorado en los datos de prueba y visualiza las predicciones.\n","\n","Datos:\n","\n","Tengo una carpeta zip en local, tambien la puse en colab pero no se si se puedan leer asi.\n","\n","El path de colab es: /content/drive/My Drive/ITD/Int Artificial Avanzada/Parte 2/Visión por computadora/Imagenes Colab/Señales de Transito/GTSRB-Training_fixed\n","\n","El path local es: \"C:\\Users\\elias\\Documents\\AI\\Parte 2 que no pude hacer en colab\\Transito\\GTSRB-Training_fixed.zip\"\n","\n","Dentro del zip:\n","+ Carpeta: GTSRB\n","+ + Dentro de GTSRB -> Carpeta: Training\n","+ Dentro de Training -> Carpetas: 00000, 00001 ... (hasta la 00042)\n","+ + Dentro de cada carpeta 000 -> 00000_00000.ppm (hasta el 00000_00029.ppm) (luego empieza 00001_00000.ppm, se repite hasta el 00004_00029.ppm) (este ejemplo aplica para el primer archivo, otros tienen mas o menos imagenes ppm)"],"metadata":{"id":"PouupgQQyfMC"}},{"cell_type":"markdown","source":["# Librerias"],"metadata":{"id":"je6X4NxPgVUb"}},{"cell_type":"code","source":["!pip install pytorch-lightning\n","!pip install torchmetrics\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install pytorch-lightning torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3s-7kC0hbPT","executionInfo":{"status":"ok","timestamp":1730249172649,"user_tz":360,"elapsed":12316,"user":{"displayName":"Elías Joaquín Yáñez Huerta","userId":"11326424779274297318"}},"outputId":"8ca57826-e4da-46af-9bf7-a7044403844a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.0+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.5.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.8)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.16.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.5.1)\n","Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.5.1)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.0+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.8)\n","Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.16.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n"]}]},{"cell_type":"code","source":["import zipfile\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","import shutil\n","import random\n","import pytorch_lightning as pl\n","from torchmetrics import Accuracy\n","import torchmetrics\n","import torch.optim as optim\n","from torchmetrics.classification import Accuracy\n","from torchvision import datasets, transforms"],"metadata":{"id":"u-ln9YWMJrKm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cargar imagenes"],"metadata":{"id":"fNcUe4zrgai7"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","zip_path = \"/content/drive/My Drive/ITD/Int Artificial Avanzada/Parte 2/Visión por computadora/Imagenes Colab/Señales de Transito/GTSRB-Training_fixed.zip\"\n","\n","# Descomprimir el archivo zip\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(\"/content/GTSRB\")\n","\n","# Ruta a la carpeta de imágenes descomprimidas\n","data_dir = \"/content/GTSRB/GTSRB/Training\"\n","\n","# Obtener la lista de clases (carpetas)\n","class_folders = sorted(os.listdir(data_dir))\n","\n","# Mostrar una imagen de cada clase\n","plt.figure(figsize=(20, 20))\n","for i, class_folder in enumerate(class_folders):\n","    # Ruta a la carpeta de la clase\n","    class_path = os.path.join(data_dir, class_folder)\n","\n","    # Verificar si class_path es un directorio antes de intentar listar su contenido\n","    if os.path.isdir(class_path):\n","        # Obtener la lista de imágenes en la carpeta de la clase\n","        image_files = os.listdir(class_path)\n","\n","        # Filtrar solo los archivos de imagen (por ejemplo, .ppm, .png, .jpg, .jpeg)\n","        image_files = [f for f in image_files if f.lower().endswith(('.ppm', '.png', '.jpg', '.jpeg'))]\n","\n","        # Verificar si hay archivos de imagen en la carpeta\n","        if image_files:\n","            # Ruta a la primera imagen de la clase\n","            image_path = os.path.join(class_path, image_files[0])\n","\n","            # Leer la imagen\n","            image = mpimg.imread(image_path)\n","\n","            # Mostrar la imagen en un subplot\n","            plt.subplot(7, 7, i + 1)\n","            plt.imshow(image)\n","            plt.title(f\"Clase {class_folder}\")\n","            plt.axis(\"off\")\n","        else:\n","            print(f\"No image files found in {class_folder}\")\n","    else:\n","        print(f\"Skipping {class_folder} as it is not a directory.\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":875,"output_embedded_package_id":"12k3TrL3BThvgmt6kSTIkQahzqc0-9NyD"},"id":"BZ0Js-PMGK8B","executionInfo":{"status":"ok","timestamp":1730249209661,"user_tz":360,"elapsed":37030,"user":{"displayName":"Elías Joaquín Yáñez Huerta","userId":"11326424779274297318"}},"outputId":"f5372049-e05e-410c-b708-1af43682d14f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Separar Conjunto de imagenes"],"metadata":{"id":"8hCTcxWsggav"}},{"cell_type":"code","source":["# Ruta a la carpeta principal del dataset\n","data_dir = \"/content/GTSRB/GTSRB/Training\"\n","\n","# Crear las carpetas train, val y test\n","os.makedirs(os.path.join(data_dir, \"train\"), exist_ok=True)\n","os.makedirs(os.path.join(data_dir, \"val\"), exist_ok=True)\n","os.makedirs(os.path.join(data_dir, \"test\"), exist_ok=True)\n","\n","# Iterar sobre las clases\n","for class_folder in os.listdir(data_dir):\n","    # Verificar si es un directorio y si NO es uno de los directorios train, val o test\n","    if os.path.isdir(os.path.join(data_dir, class_folder)) and class_folder not in [\"train\", \"val\", \"test\"]:\n","        # Obtener la lista de imágenes en la carpeta de la clase\n","        image_files = os.listdir(os.path.join(data_dir, class_folder))\n","\n","        # Mezclar las imágenes aleatoriamente\n","        random.shuffle(image_files)\n","\n","        # Calcular los índices para la separación\n","        train_split = int(0.8 * len(image_files))\n","        val_split = int(0.9 * len(image_files))\n","\n","        # Mover las imágenes a las carpetas correspondientes\n","        for i, image_file in enumerate(image_files):\n","            source_path = os.path.join(data_dir, class_folder, image_file)\n","            if i < train_split:\n","                destination_path = os.path.join(data_dir, \"train\", class_folder, image_file)\n","            elif i < val_split:\n","                destination_path = os.path.join(data_dir, \"val\", class_folder, image_file)\n","            else:\n","                destination_path = os.path.join(data_dir, \"test\", class_folder, image_file)\n","\n","            # Crear la carpeta de la clase en el destino si no existe\n","            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n","\n","            # Mover la imagen\n","            shutil.move(source_path, destination_path)\n","\n","# Contar las imágenes en cada conjunto\n","train_count = sum([len(files) for r, d, files in os.walk(os.path.join(data_dir, \"train\"))])\n","val_count = sum([len(files) for r, d, files in os.walk(os.path.join(data_dir, \"val\"))])\n","test_count = sum([len(files) for r, d, files in os.walk(os.path.join(data_dir, \"test\"))])\n","\n","# Imprimir la cantidad de imágenes en cada conjunto\n","print(f\"Cantidad de imágenes en el conjunto de entrenamiento: {train_count}\")\n","print(f\"Cantidad de imágenes en el conjunto de validación: {val_count}\")\n","print(f\"Cantidad de imágenes en el conjunto de prueba: {test_count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHLSGnRXcCae","executionInfo":{"status":"ok","timestamp":1730249209661,"user_tz":360,"elapsed":37,"user":{"displayName":"Elías Joaquín Yáñez Huerta","userId":"11326424779274297318"}},"outputId":"14b26828-ebac-4609-8e85-58d5a8ed1190"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad de imágenes en el conjunto de entrenamiento: 48517\n","Cantidad de imágenes en el conjunto de validación: 4655\n","Cantidad de imágenes en el conjunto de prueba: 5149\n"]}]},{"cell_type":"code","source":["def count_class_images(data_dir):\n","    class_counts = {}\n","    for class_folder in os.listdir(data_dir):\n","        if os.path.isdir(os.path.join(data_dir, class_folder)):\n","            class_counts[class_folder] = len(os.listdir(os.path.join(data_dir, class_folder)))\n","    return class_counts\n","\n","# Contar las imágenes por clase en cada conjunto\n","train_class_counts = count_class_images(os.path.join(data_dir, \"train\"))\n","val_class_counts = count_class_images(os.path.join(data_dir, \"val\"))\n","test_class_counts = count_class_images(os.path.join(data_dir, \"test\"))\n","\n","# Imprimir los resultados\n","print(\"Distribución de clases en el conjunto de entrenamiento:\", train_class_counts)\n","print(\"Distribución de clases en el conjunto de validación:\", val_class_counts)\n","print(\"Distribución de clases en el conjunto de prueba:\", test_class_counts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JxLF2eQQm91H","executionInfo":{"status":"ok","timestamp":1730249209661,"user_tz":360,"elapsed":34,"user":{"displayName":"Elías Joaquín Yáñez Huerta","userId":"11326424779274297318"}},"outputId":"a0aa8751-d00f-4088-9df1-a4c22e7b5109"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Distribución de clases en el conjunto de entrenamiento: {'00013': 1371, '00001': 1446, '00040': 233, '00012': 1347, '00019': 143, 'test': 35, '00021': 234, '00039': 202, '00003': 929, 'val': 41, '00015': 396, '00042': 174, '00030': 291, '00000': 143, '00022': 262, '00007': 924, '00016': 292, '00011': 860, '00034': 290, '00004': 1272, '00041': 171, '00026': 403, '00020': 233, '00035': 781, '00017': 720, '00008': 910, '00036': 262, '00033': 465, '00014': 521, '00038': 1333, '00023': 288, '00025': 982, '00006': 289, '00032': 172, '00029': 168, '00024': 176, '00005': 1214, 'train': 30, '00018': 806, '00009': 982, '00031': 539, '00037': 151, '00028': 357, '00002': 1491, '00027': 178, '00010': 1338}\n","Distribución de clases en el conjunto de validación: {'00013': 144, '00001': 150, '00040': 24, '00012': 141, '00019': 15, '00021': 24, '00039': 21, '00003': 96, 'val': 2, '00015': 75, '00042': 36, '00030': 58, '00000': 28, '00022': 51, '00007': 185, '00016': 57, '00011': 172, '00034': 56, '00004': 253, '00041': 34, '00026': 79, '00020': 46, '00035': 152, '00017': 141, '00008': 178, '00036': 52, '00033': 93, '00014': 105, '00038': 269, '00023': 70, '00025': 192, '00006': 59, '00032': 32, '00029': 30, '00024': 35, '00005': 245, '00018': 156, '00009': 187, '00031': 101, '00037': 30, '00028': 66, '00002': 285, '00027': 35, '00010': 247}\n","Distribución de clases en el conjunto de prueba: {'00013': 145, '00001': 151, '00040': 25, '00012': 142, '00019': 16, 'test': 1, '00021': 46, '00039': 41, '00003': 187, 'val': 2, '00015': 82, '00042': 35, '00030': 57, '00000': 28, '00022': 54, '00007': 188, '00016': 61, '00011': 170, '00034': 60, '00004': 251, '00041': 35, '00026': 82, '00020': 47, '00035': 158, '00017': 144, '00008': 188, '00036': 55, '00033': 91, '00014': 106, '00038': 266, '00023': 72, '00025': 198, '00006': 57, '00032': 37, '00029': 36, '00024': 38, '00005': 241, '00018': 156, '00009': 184, '00031': 107, '00037': 31, '00028': 70, '00002': 286, '00027': 33, '00010': 259}\n"]}]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Resize images to 32x32\n","    transforms.ToTensor(),        # Convert images to PyTorch tensors\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n","])\n","\n","# Create datasets for train, validation, and test\n","train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"train\"), transform=transform)\n","val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"val\"), transform=transform)\n","test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=transform)\n","\n","# Create dataloaders\n","dataloaders = {\n","    'train': DataLoader(train_dataset, batch_size=32, shuffle=True),\n","    'val': DataLoader(val_dataset, batch_size=32, shuffle=False),\n","    'test': DataLoader(test_dataset, batch_size=32, shuffle=False)  # Add test dataloader\n","}"],"metadata":{"id":"Y2eaLSnb3UTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pytorch_lightning as pl\n","from torchmetrics.classification import Accuracy\n","\n","class ImageClassifier(pl.LightningModule):\n","    def __init__(self, model, learning_rate=1e-3):\n","        super().__init__()\n","        self.model = model\n","        self.learning_rate = learning_rate\n","        self.accuracy = Accuracy(task=\"multiclass\", num_classes=43)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('train_loss', loss)\n","        self.log('train_acc', self.accuracy(logits, y))\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('val_loss', loss)\n","        self.log('val_acc', self.accuracy(logits, y))\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","\n","# Instantiate the model and LightningModule\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 43)\n","classifier = ImageClassifier(model)\n","\n","# Instantiate the Trainer\n","trainer = pl.Trainer(max_epochs=10, accelerator=\"gpu\", devices=1)\n","\n","# Train the model\n","trainer.fit(classifier, dataloaders['train'], dataloaders['val'])\n","\n","# Evaluate the model\n","def evaluate_model(classifier, dataloader, device):\n","    model = classifier.model  # Access the underlying model from the classifier\n","    model.eval()  # Set the model to evaluation mode\n","    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=43).to(device)  # Move accuracy metric to device and match configuration\n","    with torch.no_grad():  # Do not calculate gradients during evaluation\n","        for data in dataloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = model(images)\n","            # Ensure labels have the correct data type\n","            labels = labels.type(torch.LongTensor).to(device)  # Move labels to device and match configuration\n","            accuracy.update(outputs, labels)  # Update accuracy metric\n","    return accuracy.compute().item()  # Get the accuracy value\n","\n","# Assuming 'dataloaders' is already defined (from your previous code)\n","accuracy = evaluate_model(classifier, dataloaders['val'], device='cuda' if torch.cuda.is_available() else 'cpu')  # Specify device\n","print(f'Accuracy on validation set: {accuracy:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"Ed_UXeZbeJXE","executionInfo":{"status":"error","timestamp":1730250130758,"user_tz":360,"elapsed":372,"user":{"displayName":"Elías Joaquín Yáñez Huerta","userId":"11326424779274297318"}},"outputId":"7ff33f4d-f413-4164-c8e2-8801f9776619"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-48893b60c387>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Mueve el modelo a la GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Instantiate a PyTorch Lightning Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/device_dtype_mixin.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0m_update_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","classifier.to(device)\n","classifier.eval()  # Set the model to evaluation mode\n","\n","# Get a batch of images and labels from the test dataloader\n","dataiter = iter(dataloaders['test'])\n","images, labels = next(dataiter)\n","\n","# Move images and labels to the device\n","images = images.to(device)\n","labels = labels.to(device)\n","\n","# Get predictions from the model\n","outputs = classifier(images)\n","_, predicted = torch.max(outputs, 1)\n","\n","# Display a few images with labels and predictions\n","num_images_to_display = 2  # Adjust as needed\n","\n","fig, axes = plt.subplots(1, num_images_to_display, figsize=(10, 5))\n","\n","for i in range(num_images_to_display):\n","    # Convert image tensor to numpy array for display\n","    image = images[i].cpu().permute(1, 2, 0).numpy()\n","    image = (image * 255).astype(np.uint8)  # Scale to 0-255\n","\n","    # Get the true label and predicted label\n","    true_label = class_folders[labels[i].item()]\n","    predicted_label = class_folders[predicted[i].item()]\n","\n","    # Display the image with labels\n","    axes[i].imshow(image)\n","    axes[i].set_title(f\"True: {true_label}\\nPredicted: {predicted_label}\")\n","    axes[i].axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"mexDBhFQjZe-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definir las transformaciones\n","transform = transforms.Compose([\n","    transforms.RandomRotation(degrees=(-15, 15)),\n","    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar las imágenes\n","])\n","\n","# Crear el dataset con las transformaciones\n","train_dataset = datasets.ImageFolder(root='path/to/train/data', transform=transform)\n","\n","# Crear el dataloader\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"76tm2YzXqMxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtener un batch de imágenes del dataset de prueba\n","images, labels = next(iter(test_dataloader))\n","\n","# Mover las imágenes y etiquetas al dispositivo (GPU si está disponible)\n","images = images.to(device)\n","labels = labels.to(device)\n","\n","# Obtener las predicciones del modelo\n","with torch.no_grad():\n","  outputs = model(images)\n","  _, predicted = torch.max(outputs, 1)\n","\n","# Mostrar algunas imágenes con sus etiquetas reales y predicciones\n","num_images_to_show = 5  # Número de imágenes a mostrar\n","fig, axes = plt.subplots(1, num_images_to_show, figsize=(15, 3))\n","for i in range(num_images_to_show):\n","  # Convertir la imagen de tensor a numpy array\n","  image = images[i].cpu().numpy()\n","  image = np.transpose(image, (1, 2, 0))  # Cambiar el orden de los canales\n","\n","  # Mostrar la imagen\n","  axes[i].imshow(image)\n","  axes[i].set_title(f\"Real: {class_names[labels[i]]}\\nPred: {class_names[predicted[i]]}\")\n","  axes[i].axis(\"off\")\n","\n","plt.show()"],"metadata":{"id":"-kWKUZdXrYqY"},"execution_count":null,"outputs":[]}]}