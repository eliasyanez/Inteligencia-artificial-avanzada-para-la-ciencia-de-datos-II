{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "cuantas veces aparecen los puntos,\n",
        "\n",
        "las palabras con poco contenido semantico"
      ],
      "metadata": {
        "id": "C3w4LD5jRRh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = \"/content/drive/My Drive/ITD/Int Artificial Avanzada/Parte 2/PLN/NLTK/\"\n",
        "\n",
        "# Para visualizar originallTraining.txt\n",
        "with open(os.path.join(base_path, \"originallTraining.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in range(10):  # Imprime las primeras 10 líneas\n",
        "        line = f.readline()\n",
        "import os\n",
        "\n",
        "base_path = \"/content/drive/My Drive/ITD/Int Artificial Avanzada/Parte 2/PLN/NLTK/\"\n",
        "\n",
        "# Para visualizar originallTraining.txt\n",
        "with open(os.path.join(base_path, \"originallTraining.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in range(10):  # Imprime las primeras 10 líneas\n",
        "        line = f.readline()\n",
        "        print(line)\n",
        "\n",
        "with open(os.path.join(base_path, \"originalTest.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzSFsS6Zv19U",
        "outputId": "9cfb62ea-6c6c-4337-fbc3-f94ff25a7062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "264183816548130816\t15140428\tpositive\tGas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\n",
            "\n",
            "264249301910310912\t18516728\tnegative\tIranian general says Israel's Iron Dome can't deal with their missiles (keep talking like that and we may end up finding out)\n",
            "\n",
            "264105751826538497\t147088367\tpositive\twith J Davlar 11th. Main rivals are team Poland. Hopefully we an make it a successful end to a tough week of training tomorrow.\n",
            "\n",
            "264094586689953794\t332474633\tnegative\tTalking about ACT's && SAT's, deciding where I want to go to college, applying to colleges and everything about college stresses me out.\n",
            "\n",
            "254941790757601280\t557103111\tnegative\tThey may have a SuperBowl in Dallas, but Dallas ain't winning a SuperBowl. Not with that quarterback and owner. @S4NYC @RasmussenPoll\n",
            "\n",
            "264169034155696130\t382403760\tneutral\tIm bringing the monster load of candy tomorrow, I just hope it doesn't get all squiched\n",
            "\n",
            "263192091700654080\t344222239\tobjective-OR-neutral\tApple software, retail chiefs out in overhaul: SAN FRANCISCO Apple Inc CEO Tim Cook on Monday replaced the heads... http://bit.ly/XQEhJU \n",
            "\n",
            "260200142420992000\t332530284\tobjective\t#Livewire Nadal confirmed for Mexican Open in February: Rafael Nadal is set to play at the Me... http://bit.ly/WY4Vjy  #LiveWireAthletics\n",
            "\n",
            "263304719471087617\t564843841\tobjective\t#Iran US delisting MKO from global terrorists list in line with Iran campaign: Tehran, Oct 30, IRNA -- Secretary... http://bit.ly/XSTGtd \n",
            "\n",
            "263975113404342273\t616166780\tobjective\tExpect light-moderate rains over E. Visayas; Cebu, Bohol, Samar & Leyte have 30-70% chance of rains tonight! Expect fair weather tomorrow!:)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realice lo siguiente sobre el archivo sin procesar de tweets:\n",
        "\n",
        "1. Proponga y describa detalle un proceso de preprocesamiento sobre “textos crudos\" que maximice el vocabulario en un modelo de aprendizaje supervisado. El proceso debe incluir:\n",
        "+ Codificación/decodificación (si aplica)\n",
        "+ Manejo de mayúsculas y minúsculas\n",
        "+ Uso de patrones Regex\n",
        "+ Manejo de palabras cerradas\n",
        "+ Manejo de signos de puntuación y caracteres especiales\n",
        "+ Manejo de etiquetas del discurso (PoS Tags)\n",
        "+ Uso de un lematizador/stemming.\n",
        "+ Entre otros\n",
        "2. Utilizando el pipeline propuesto, limpie el texto y ocupe alguna de las técnicas de aprendizaje supervisado vistas anteriormente (Naive Bayes, por ejemplo) con una representación básica como Bag of words o One-hot-encoding (también vistas de manera elemental antes) para entrenar un modelo de análisis de sentimientos.\n",
        "3. Documente distintas pruebas para ver si el uso de un preprocesamiento mejora o empeora el rendimiento de un nuevo modelo.\n",
        "4. Muestre las conclusiones alcanzadas desde el punto de vista del manejo del lenguaje."
      ],
      "metadata": {
        "id": "iRN7OQH3taAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define column names\n",
        "column_names = ['id', 'user_id', 'sentiment', 'text']\n",
        "\n",
        "# Read training data\n",
        "training_data = pd.read_csv(os.path.join(base_path, \"originallTraining.txt\"),\n",
        "                            sep='\\t', header=None, names=column_names)\n",
        "\n",
        "# Read test data (assuming similar format)\n",
        "test_data = pd.read_csv(os.path.join(base_path, \"originalTest.txt\"),\n",
        "                        sep='\\t', header=None, names=column_names)\n",
        "\n",
        "# Now you can access columns by name:\n",
        "training_data['text'] # Access the text column\n",
        "training_data['sentiment'] # Access the sentiment column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "QgE1PDJHx0Do",
        "outputId": "371b9284-a785-49f2-c879-6d51daf257c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       positive\n",
              "1       negative\n",
              "2       positive\n",
              "3       negative\n",
              "4       negative\n",
              "          ...   \n",
              "6147    negative\n",
              "6148    positive\n",
              "6149    positive\n",
              "6150     neutral\n",
              "6151    negative\n",
              "Name: sentiment, Length: 6152, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6147</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6148</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6149</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6150</th>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6151</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6152 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from scipy.sparse import hstack\n",
        "import pandas as pd\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.matutils import corpus2csc"
      ],
      "metadata": {
        "id": "PzrO0QuQ_fl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
        "    return hashtags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob8t9cxm62de",
        "outputId": "c4b19c51-12a4-45ab-ea48-1604d98a1e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define column names\n",
        "column_names = ['id', 'user_id', 'sentiment', 'text']\n",
        "\n",
        "# Read training data\n",
        "base_path = \"/content/drive/My Drive/ITD/Int Artificial Avanzada/Parte 2/PLN/NLTK/\"\n",
        "training_data = pd.read_csv(os.path.join(base_path, \"originallTraining.txt\"),\n",
        "                            sep='\\t', header=None, names=column_names)\n",
        "\n",
        "# Read test data (assuming similar format)\n",
        "test_data = pd.read_csv(os.path.join(base_path, \"originalTest.txt\"),\n",
        "                        sep='\\t', header=None, names=column_names)\n",
        "\n",
        "# Truncate datasets to ensure equal length\n",
        "min_length = min(len(training_data), len(test_data))\n",
        "training_data = training_data[:min_length]\n",
        "test_data = test_data[:min_length]\n",
        "\n",
        "# Preprocesar los datos de entrenamiento y prueba\n",
        "training_data['processed_text'] = training_data['text'].apply(preprocess_text)\n",
        "test_data['processed_text'] = test_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Extraer hashtags\n",
        "training_data['hashtags'] = training_data['text'].apply(extract_hashtags)\n",
        "test_data['hashtags'] = test_data['text'].apply(extract_hashtags)"
      ],
      "metadata": {
        "id": "HFXLCRxp_sdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear diccionario y Bag of Words con Gensim\n",
        "texts = [text.split() for text in training_data['processed_text']]\n",
        "dictionary = Dictionary(texts)\n",
        "training_data_bow = [dictionary.doc2bow(text.split()) for text in training_data['processed_text']]\n",
        "test_data_bow = [dictionary.doc2bow(text.split()) for text in test_data['processed_text']]\n",
        "\n",
        "# Convertir a matriz dispersa\n",
        "training_data_vectorized = corpus2csc(training_data_bow, num_terms=len(dictionary)).transpose()\n",
        "test_data_vectorized = corpus2csc(test_data_bow, num_terms=len(dictionary)).transpose()\n",
        "# Crear características de hashtags\n",
        "mlb = MultiLabelBinarizer()\n",
        "hashtag_features_train = mlb.fit_transform(training_data['hashtags'])\n",
        "hashtag_features_test = mlb.transform(test_data['hashtags'])\n",
        "\n",
        "# Combinar características\n",
        "training_data_vectorized = hstack([training_data_vectorized, hashtag_features_train])\n",
        "test_data_vectorized = hstack([test_data_vectorized, hashtag_features_test])\n",
        "\n",
        "# Definir los hiperparámetros a ajustar\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1.0, 10.0],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "# Assuming 'sentiment' is the target variable\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    training_data_vectorized, training_data['sentiment'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZWF-o7YAVop",
        "outputId": "5a243545-e194-4b5a-9b8c-72b158167ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['1game', '1st', '299COMM', '3', '49ers', '4N', '4networking', '5yearsold', '7FactsAboutMyBestFriend', '7ThingsAboutMyBestFriend', 'ACC', 'AFC', 'Arsenal', 'AtrAin', 'BBAU', 'Badtime', 'Believe', 'BestPrimetimeLineup', 'BringBackLMFAO', 'Brownlow2012', 'Bulldogs', 'Bullshifter', 'ByeByeWage', 'CISwrug12', 'Celtics', 'Charlie_Chaplin', 'Clippers', 'Come', 'Contraband', 'CurrentEvents', 'DECALove', 'DeathValley', 'E60', 'EXCITED', 'Excited', 'F2S', 'FAN', 'FIFA13', 'FIREPIOLI', 'Facts', 'FamOverAll', 'FightForMatt', 'FirstTake', 'Flight', 'FootballAsAKid', 'FreeBooth', 'GGMU', 'Generous', 'GoBears', 'GoBroncos', 'GoIndiana', 'GoodTimes', 'Gossip', 'GreatSong', 'HOLLA', 'HOU', 'HalloweenSong', 'HamOnt', 'Happy', 'HappyBirthdayTroianBellisario', 'HarryPotter', 'HatedIt', 'HavePlenty', 'Hawks', 'HerdNation', 'Hokies', 'HowSad', 'ILoveMyBestFriend', 'Idiots', 'Inspiration', 'Intersex', 'January', 'JustinDoAFollowSpree', 'KGA', 'Kstate', 'LA', 'LFC', 'LSU', 'Lakers', 'LakersPride', 'LittleThngs', 'MLSPlayoffs', 'MMA', 'MadeMyNight', 'Madonna', 'Mention20PeopleYouWantToMeet', 'MichaelVick', 'Monday', 'NAIAMBB', 'NCSU', 'NHL', 'NUFC', 'NW', 'NoRide', 'NotreDame', 'PA4', 'PLEAAASSEEEEE', 'PROMO', 'Patriots', 'PitBull', 'Please', 'Politics', 'Portland', 'QSU', 'RAW', 'REVtv', 'RHAP', 'Raptors', 'RazorBacks', 'RefereeUnited', 'Repost', 'SEAvRSL', 'SEC', 'SMTA', 'SNC', 'Saddd', 'Sandy', 'Senate', 'Skyliners', 'SparkingAt100Mph', 'SpeedKills', 'SunTran', 'SupportLocalTalent', 'Surveillance', 'TOPSTORIES', 'TV', 'TVDUKFamily', 'TalkLikeYourBestFriend', 'TeamFollowBack', 'TeamLeah', 'TeamUSA', 'Tebowmania', 'TestifyhisGreatness', 'TheAKAdemyAwards', 'TheBachelor', 'TheBay', 'TheBlaze', 'TheRealRomney', 'Toronto', 'Trending', 'TrueBitches', 'Truth', 'UFC', 'UberSoccer', 'VH1', 'WEB', 'WIFLC', 'WeAreND', 'WeLoveThai', 'WhitecapsFC', 'WorkToDo', 'Yelp', 'YourHealthIsYour', 'amazing', 'badfever', 'bama', 'bamf', 'banker', 'bestcelebcouple', 'bestintheworld', 'bestshow', 'bitterness', 'blunted', 'bookofquotes', 'bored', 'bostongirl', 'brockuproblems', 'bummer', 'burningdaylight', 'c', 'career', 'cfc', 'cheerpracticeonhalloween', 'citychick', 'cleveland', 'clutch', 'concert', 'confirmed', 'conservative', 'consistency', 'crushed', 'curses', 'daboswinneyproblems', 'deadline', 'delayed', 'dontshopadopt', 'drunken', 'ephrata', 'fact', 'fantasyfootball', 'fanupmu', 'feedmemore', 'ff', 'fireworks', 'firstworldpains', 'fml', 'focus', 'foursquare', 'friday', 'fuckkkk', 'getin', 'girlpower', 'gopies', 'heavyweights', 'herbaddiction', 'hi', 'hopethisistrue', 'hustle', 'indianamensbasetball', 'ineedmoregirlfriends', 'infosec', 'iowacheer', 'iowafootball', 'isheturnedornot', 'iusocc', 'iwishbrodywouldwearone', 'january', 'jeal', 'killmenow', 'letsgohawk', 'lizlemon', 'love', 'lovethem', 'missher', 'mixitup', 'mkdons', 'mmm', 'motown', 'movienews', 'music', 'ncaa', 'nffc', 'nightsinfrontofthetelly', 'njed', 'noshame', 'notupinhere', 'oldham', 'pageantday', 'pasoccer', 'pathetic', 'pause', 'photos', 'ptsafety', 'quote', 'roadtrippin', 'roses', 'santaanita', 'seahawks', 'sec', 'sfc', 'sidebyside', 'sinsavers', 'smartbaby', 'soproud', 'spoiled', 'sports', 'stdlav', 'stopit', 'sunsetcinema', 'swfc', 'thankyouTony', 'thinkingofthefans', 'time', 'toofar', 'train', 'trynnaturnthisdayaround', 'utpol', 'verysickgirl', 'vision', 'will', 'work', 'wreckemtech', 'xfactor'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un clasificador Naive Bayes\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Realizar la búsqueda de cuadrícula para encontrar los mejores hiperparámetros\n",
        "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
        "grid_search.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Imprimir los mejores hiperparámetros encontrados\n",
        "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluar el modelo con los mejores hiperparámetros\n",
        "best_classifier = grid_search.best_estimator_\n",
        "y_pred = best_classifier.predict(X_val_split)\n",
        "accuracy = accuracy_score(y_val_split, y_pred)\n",
        "print(f'Precisión del modelo: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr_lTigFD1vV",
        "outputId": "4aa3b43b-8c69-42b6-a2ea-3c37b5420b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores hiperparámetros: {'alpha': 10.0, 'fit_prior': False}\n",
            "Precisión del modelo: 41.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir las etiquetas para los datos de prueba\n",
        "y_pred = best_classifier.predict(test_data_vectorized) # Replace X_test with test_data_vectorized\n",
        "\n",
        "# Mostrar algunas predicciones con su información real\n",
        "for i in range(10):  # Mostrar las primeras 10 predicciones\n",
        "    print(f\"Texto: {test_data['text'][i]}\")\n",
        "    print(f\"Sentimiento real: {test_data['sentiment'][i]}\")\n",
        "    print(f\"Sentimiento predicho: {y_pred[i]}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXad6e3DzOXC",
        "outputId": "45aa79a9-674a-4d19-e3a6-6047561e4039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: Won the match #getin . Plus, tomorrow is a very busy day, with Awareness Day's and debates. Gulp. Debates...\n",
            "Sentimiento real: neutral\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: Lunch from my new Lil spot ...THE COTTON BOWL ....pretty good#1st#time#will be going back# http://instagr.am/p/RX9939CIv8/ \n",
            "Sentimiento real: positive\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: SNC Halloween Pr. Pumped. Let's work it for Sunday....Packers vs....who knows or caresn. #SNC #cheerpracticeonhalloween\n",
            "Sentimiento real: positive\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: Manchester United will try to return to winning ways when they face Arsenal in the Premier League at Old Trafford on Saturday.\n",
            "Sentimiento real: neutral\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: Going to a bulls game with Aaliyah & hope next Thursday\n",
            "Sentimiento real: neutral\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: Any Toon Fans with a spare ticket for Anfield on Sunday?willing to pay extra #NUFC\n",
            "Sentimiento real: neutral\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: Louis inspired outfit on Monday and Zayn inspired outfit today..4/5 done just need Harry :)\n",
            "Sentimiento real: positive\n",
            "Sentimiento predicho: neutral\n",
            "--------------------\n",
            "Texto: going to bed now...Rose parade then game tomorrow\n",
            "Sentimiento real: objective\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: @_Nenaah oh cause my friend got something from china and they said it will take at least 6 to 8 weeks and it came in the 2nd week :P\n",
            "Sentimiento real: neutral\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n",
            "Texto: I love the banner that was unfurled in the United end last night. It read: \"Chelsea - Standing up against racism since Sunday\"\n",
            "Sentimiento real: positive\n",
            "Sentimiento predicho: positive\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones sobre el manejo del lenguaje en el análisis de sentimientos:\n",
        "\n",
        "**El preprocesamiento del texto es crucial para el rendimiento del modelo:**\n",
        "\n",
        "+ La eliminación de signos de puntuación, caracteres especiales y la conversión a minúsculas ayuda a estandarizar el texto y reduce el ruido en los datos.\n",
        "+ La eliminación de palabras vacías (stop words) elimina términos comunes que no aportan mucha información al análisis de sentimientos.\n",
        "+ La lematización reduce las palabras a su forma raíz, lo que ayuda a agrupar palabras con significado similar y mejora la precisión del modelo.\n",
        "\n",
        "**La representación Bag of Words es una técnica básica pero efectiva:**\n",
        "\n",
        "+ Permite convertir el texto en una representación numérica que el modelo puede entender.\n",
        "+ Sin embargo, puede perder información sobre el orden de las palabras y el contexto en el que aparecen.\n",
        "\n",
        "**El modelo Naive Bayes es simple pero puede ser efectivo para el análisis de sentimientos:**\n",
        "+ Es fácil de implementar y entrenar.\n",
        "+ Sin embargo, puede ser menos preciso que otros modelos más complejos, como las redes neuronales.\n",
        "\n",
        "**La precisión del modelo puede mejorarse con un preprocesamiento más avanzado:**\n",
        "+ Se podrían explorar otras técnicas de preprocesamiento como la corrección ortográfica, la detección de entidades nombradas y el análisis de dependencias.\n",
        "+ También se podrían utilizar representaciones de texto más avanzadas como word embeddings o modelos de lenguaje pre-entrenados.\n",
        "\n",
        "**En general, el manejo adecuado del lenguaje es fundamental para obtener buenos resultados en el análisis de sentimientos:**\n",
        "\n",
        "+ El preprocesamiento del texto ayuda a limpiar y estandarizar los datos.\n",
        "+ La elección de la representación del texto y el modelo de aprendizaje afecta la precisión del modelo.\n",
        "+ La experimentación y la evaluación son esenciales para encontrar la mejor configuración para una tarea específica.\n",
        "\n",
        "**En este caso particular, el modelo logró una precisión de alrededor del 41.54%, lo que indica que hay margen de mejora:**\n",
        "+ Se podrían explorar técnicas de preprocesamiento más avanzadas y diferentes modelos de aprendizaje para aumentar la precisión.\n",
        "+ También sería útil analizar los errores del modelo para entender mejor sus limitaciones y encontrar formas de mejorar su rendimiento."
      ],
      "metadata": {
        "id": "HFDCQhSY1AJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar training_data en un archivo CSV\n",
        "training_data.to_csv(os.path.join(base_path, \"training_data_processed.csv\"), index=False)\n",
        "\n",
        "# Guardar test_data en un archivo CSV\n",
        "test_data.to_csv(os.path.join(base_path, \"test_data_processed.csv\"), index=False)"
      ],
      "metadata": {
        "id": "5uoaGAeGuyUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}